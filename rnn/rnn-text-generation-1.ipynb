{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Using Recurrent Neural Network (RNN) to process the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google COLAB :  False\n"
     ]
    }
   ],
   "source": [
    "## Determine if we are running on google colab\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    RUNNING_IN_COLAB = True\n",
    "except:\n",
    "    RUNNING_IN_COLAB = False\n",
    "\n",
    "print (\"Running in Google COLAB : \", RUNNING_IN_COLAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can used your own dataset with english text\n",
    "data_location = \"../data/text/state-of-the-unions/2009-Obama.txt\"\n",
    "\n",
    "if RUNNING_IN_COLAB:\n",
    "    data_location = 'https://elephantscale-public.s3.amazonaws.com/data/text/state-of-the-unions/2009-Obama.txt'\n",
    "    !wget $data_location  -O '2009-Obama.txt'\n",
    "    data_location = '2009-Obama.txt'\n",
    "\n",
    "    \n",
    "with open( data_location, \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(text) <class 'str'>\n",
      "len(text) :  33794\n",
      "---- text[:20]---\n",
      "Madame Speaker, Mr. \n",
      "-------\n",
      "----text[:1000]---\n",
      "Madame Speaker, Mr. Vice President, Members of Congress, and the First Lady of\n",
      "the United States:\n",
      "\n",
      "I've come here tonight not only to address the distinguished men and women in\n",
      "this great chamber, but to speak frankly and directly to the men and women who\n",
      "sent us here.\n",
      "\n",
      "I know that for many Americans watching right now, the state of our economy is\n",
      "a concern that rises above all others.  And rightly so.  If you haven't been\n",
      "personally affected by this recession, you probably know someone who has -- a\n",
      "friend; a neighbor; a member of your family.  You don't need to hear another\n",
      "list of statistics to know that our economy is in crisis, because you live it\n",
      "every day.  It's the worry you wake up with and the source of sleepless\n",
      "nights.  It's the job you thought you'd retire from but now have lost; the\n",
      "business you built your dreams upon that's now hanging by a thread; the\n",
      "college acceptance letter your child had to put back in the envelope.  The\n",
      "impact of this recession is real, and it is ev\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "print ('type(text)', type(text))\n",
    "print(\"len(text) : \", len(text))\n",
    "print('---- text[:20]---')\n",
    "print(text[:20])\n",
    "print('-------')\n",
    "print('----text[:1000]---')\n",
    "print(text[:1000])\n",
    "print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Shape Data\n",
    "\n",
    "### 3.1 - Remove character and create vocabulary\n",
    "\n",
    "<img src=\"../assets/images/rnn_vocab.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len :  28\n",
      "vocab :  {'p', 'l', 't', 'w', 'b', 'i', 'm', 'o', 'k', 'f', 'y', 'v', '-', 'j', ' ', 's', 'x', 'a', 'c', 'd', 'h', 'g', 'q', 'n', 'r', 'z', 'e', 'u'}\n",
      "\n",
      "text:  madame speaker mr vice president members of congress and the first lady ofthe united statesive come \n"
     ]
    }
   ],
   "source": [
    "import unidecode \n",
    "import re\n",
    "\n",
    "text = unidecode.unidecode(text)\n",
    "text = text.lower()\n",
    "\n",
    "text = re.sub(r'\\d', '', text)  # replace numbers\n",
    "text = re.sub(r'[^0-9a-zA-Z\\ \\-]+', '', text)\n",
    "text = re.sub(r'[,:\\n]+', '', text)\n",
    "text = text.strip()\n",
    "\n",
    "vocab = set(text)\n",
    "print(\"vocab len : \", len(vocab))\n",
    "print (\"vocab : \", vocab)\n",
    "\n",
    "print()\n",
    "print(\"text: \", text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unidecode \n",
    "# import re\n",
    "\n",
    "# text = unidecode.unidecode(text)\n",
    "# text = text.lower()\n",
    "\n",
    "# text = re.sub('\\d', '', text)  # replace numbers\n",
    "# text = text.replace(\">\", \"\")\n",
    "# text = text.replace(\"<\", \"\")\n",
    "# text = text.replace(\"!\", \"\")\n",
    "# text = text.replace(\"?\", \"\")\n",
    "# text = text.replace(\"-\", \"\")\n",
    "# text = text.replace(\"$\", \"\")\n",
    "# text = text.replace(\"%\", \"\")\n",
    "\n",
    "# # text = text.replace(\"'\", \"\")\n",
    "# text = text.replace(\";\", \"\")\n",
    "# text = text.replace('\"', \"\")\n",
    "# # text = text.replace(',', \"\")\n",
    "# text = text.replace('\\n', \"\")\n",
    "# text = text.replace('/', \"\")\n",
    "\n",
    "# text = text.strip()\n",
    "\n",
    "# vocab = set(text)\n",
    "# print(\"vocab len : \", len(vocab), \", vocab : \", vocab)\n",
    "# print()\n",
    "# print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Map each letter to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_to_int: \n",
      " {' ': 14,\n",
      " '-': 12,\n",
      " 'a': 17,\n",
      " 'b': 4,\n",
      " 'c': 18,\n",
      " 'd': 19,\n",
      " 'e': 26,\n",
      " 'f': 9,\n",
      " 'g': 21,\n",
      " 'h': 20,\n",
      " 'i': 5,\n",
      " 'j': 13,\n",
      " 'k': 8,\n",
      " 'l': 1,\n",
      " 'm': 6,\n",
      " 'n': 23,\n",
      " 'o': 7,\n",
      " 'p': 0,\n",
      " 'q': 22,\n",
      " 'r': 24,\n",
      " 's': 15,\n",
      " 't': 2,\n",
      " 'u': 27,\n",
      " 'v': 11,\n",
      " 'w': 3,\n",
      " 'x': 16,\n",
      " 'y': 10,\n",
      " 'z': 25}\n",
      "\n",
      "int_to_vocab: \n",
      " {0: 'p',\n",
      " 1: 'l',\n",
      " 2: 't',\n",
      " 3: 'w',\n",
      " 4: 'b',\n",
      " 5: 'i',\n",
      " 6: 'm',\n",
      " 7: 'o',\n",
      " 8: 'k',\n",
      " 9: 'f',\n",
      " 10: 'y',\n",
      " 11: 'v',\n",
      " 12: '-',\n",
      " 13: 'j',\n",
      " 14: ' ',\n",
      " 15: 's',\n",
      " 16: 'x',\n",
      " 17: 'a',\n",
      " 18: 'c',\n",
      " 19: 'd',\n",
      " 20: 'h',\n",
      " 21: 'g',\n",
      " 22: 'q',\n",
      " 23: 'n',\n",
      " 24: 'r',\n",
      " 25: 'z',\n",
      " 26: 'e',\n",
      " 27: 'u'}\n",
      "\n",
      "int for e: 26\n",
      "letter for 26: e\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_to_int = {l:i for i,l in enumerate(vocab)}\n",
    "int_to_vocab = {i:l for i,l in enumerate(vocab)}\n",
    "\n",
    "print(\"vocab_to_int: \\n\", pprint.pformat(vocab_to_int))\n",
    "print()\n",
    "print(\"int_to_vocab: \\n\", pprint.pformat(int_to_vocab))\n",
    "\n",
    "print(\"\\nint for e:\", vocab_to_int[\"e\"])\n",
    "int_for_e = vocab_to_int[\"e\"]\n",
    "print(\"letter for %s: %s\" % (vocab_to_int[\"e\"], int_to_vocab[int_for_e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(t):\n",
    "    return [vocab_to_int[l] for l in t]\n",
    "    \n",
    "def decode_text(encoded):\n",
    "    return [int_to_vocab[i] for i in encoded_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  madame speaker mr vice president members of congress and the first lady ofthe united statesive come \n",
      "\n",
      "encoded sentence :  [6, 17, 19, 17, 6, 26, 14, 15, 0, 26, 17, 8, 26, 24, 14, 6, 24, 14, 11, 5, 18, 26, 14, 0, 24, 26, 15, 5, 19, 26, 23, 2, 14, 6, 26, 6, 4, 26, 24, 15, 14, 7, 9, 14, 18, 7, 23, 21, 24, 26, 15, 15, 14, 17, 23, 19, 14, 2, 20, 26, 14, 9, 5, 24, 15, 2, 14, 1, 17, 19, 10, 14, 7, 9, 2, 20, 26, 14, 27, 23, 5, 2, 26, 19, 14, 15, 2, 17, 2, 26, 15, 5, 11, 26, 14, 18, 7, 6, 26, 14]\n"
     ]
    }
   ],
   "source": [
    "# encoded = [vocab_to_int[l] for l in text]\n",
    "encoded = encode_text(text)\n",
    "encoded_sentence = encoded[:100]\n",
    "\n",
    "print (\"original text : \", text[:100])\n",
    "print()\n",
    "print(\"encoded sentence : \", encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded sentence:  ['m', 'a', 'd', 'a', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', 'e', 'r', ' ', 'm', 'r', ' ', 'v', 'i', 'c', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'm', 'e', 'm', 'b', 'e', 'r', 's', ' ', 'o', 'f', ' ', 'c', 'o', 'n', 'g', 'r', 'e', 's', 's', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'r', 's', 't', ' ', 'l', 'a', 'd', 'y', ' ', 'o', 'f', 't', 'h', 'e', ' ', 'u', 'n', 'i', 't', 'e', 'd', ' ', 's', 't', 'a', 't', 'e', 's', 'i', 'v', 'e', ' ', 'c', 'o', 'm', 'e', ' ']\n"
     ]
    }
   ],
   "source": [
    "# decoded_sentence = [int_to_vocab[i] for i in encoded_sentence]\n",
    "decoded_sentence = decode_text(encoded_sentence)\n",
    "print(\"decoded sentence: \" , decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madame speaker mr vice president members of congress and the first lady ofthe united statesive come \n"
     ]
    }
   ],
   "source": [
    "decoded_sentence2 = \"\".join(decoded_sentence)\n",
    "print(decoded_sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Generate batch\n",
    "\n",
    "### Sample of one batch\n",
    "\n",
    "<img src=\"../assets/images/rnn_letter.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs [6, 17, 19, 17, 6, 26, 14, 15, 0, 26]\n",
      "Targets [17, 19, 17, 6, 26, 14, 15, 0, 26, 17]\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = encoded, encoded[1:]\n",
    "\n",
    "# predict the next ones\n",
    "print(\"Inputs\", inputs[:10])\n",
    "print(\"Targets\", targets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method used to generate batch in sequence order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "    # Size of each chunk\n",
    "    chuck_size = (len(inputs) -1)  // batch_size\n",
    "    # Numbef of sequence per chunk\n",
    "    sequences_per_chunk = chuck_size // seq_len\n",
    "\n",
    "    for s in range(0, sequences_per_chunk):\n",
    "        batch_inputs = np.zeros((batch_size, seq_len))\n",
    "        batch_targets = np.zeros((batch_size, seq_len))\n",
    "        for b in range(0, batch_size):\n",
    "            fr = (b*chuck_size)+(s*seq_len)\n",
    "            to = fr+seq_len\n",
    "            batch_inputs[b] = inputs[fr:to]\n",
    "            batch_targets[b] = inputs[fr+1:to+1]\n",
    "            \n",
    "            if noise > 0:\n",
    "                noise_indices = np.random.choice(seq_len, noise)\n",
    "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
    "            \n",
    "        yield batch_inputs, batch_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no noise\n",
      "[ 6. 17. 19. 17.  6.] [17. 19. 17.  6. 26.]\n",
      "with some noise\n",
      "[ 1.  1. 19. 17.  6.] [17. 19. 17.  6. 26.]\n"
     ]
    }
   ],
   "source": [
    "print ('no noise')\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=0):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break\n",
    "\n",
    "\n",
    "print ('with some noise')\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=3):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create model\n",
    "### 4.1 - Create your own layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHot(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OneHot, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the layer works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50)\n",
      "(32, 50, 28)\n",
      "Input letter is: 6.0\n",
      "One hot representation of the letter [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "class RnnModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super(RnnModel, self).__init__()\n",
    "        # Convolutions\n",
    "        self.one_hot = OneHot(len(vocab))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = self.one_hot(inputs)\n",
    "        return output\n",
    "\n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 32))\n",
    "\n",
    "print(batch_inputs.shape)\n",
    "\n",
    "model = RnnModel(len(vocab))\n",
    "output = model.predict(batch_inputs)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "#print(output)\n",
    "\n",
    "print(\"Input letter is:\", batch_inputs[0][0])\n",
    "print(\"One hot representation of the letter\", output[0][0])\n",
    "\n",
    "#assert(output[int(batch_inputs[0][0])]==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Setup the model\n",
    "\n",
    "<img src=\"../assets/images/architecture_rnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "### Creat the layers\n",
    "\n",
    "# Set the input of the model\n",
    "tf_inputs = tf.keras.Input(shape=(None,), batch_size=64)\n",
    "# Convert each value of the  input into a one encoding vector\n",
    "one_hot = OneHot(len(vocab))(tf_inputs)\n",
    "# Stack LSTM cells\n",
    "rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot)\n",
    "rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
    "# Create the outputs of the model\n",
    "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(rnn_layer2)\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "### Setup the model\n",
    "model = tf.keras.Model(inputs=tf_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Check if we can reset the RNN cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star by resetting the cells of the RNN\n",
    "model.reset_states()\n",
    "\n",
    "# Get one batch\n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 64))\n",
    "\n",
    "# Make a first prediction\n",
    "outputs = model.predict(batch_inputs)\n",
    "first_prediction = outputs[0][0]\n",
    "\n",
    "# Reset the states of the RNN states\n",
    "model.reset_states()\n",
    "\n",
    "# Make an other prediction to check the difference\n",
    "outputs = model.predict(batch_inputs)\n",
    "second_prediction = outputs[0][0]\n",
    "\n",
    "# Check if both prediction are equal\n",
    "assert(set(first_prediction)==set(second_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Set the loss and objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Set some metrics to track the progress of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# Accuracy\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Set the train method and the predict method in graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(inputs)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_object(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def predict(inputs):\n",
    "    # Make a prediction on all the batch\n",
    "    predictions = model(inputs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "\n",
    "for epoch in range(4000):\n",
    "    for batch_inputs, batch_targets in gen_batch(inputs, targets, 100, 64, noise=13):\n",
    "        train_step(batch_inputs, batch_targets)\n",
    "    template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
    "    print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source:\n",
    "https://github.com/thibo73800/tensorflow2.0-examples/blob/master/RNN%20-%20Text%20Generator.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
