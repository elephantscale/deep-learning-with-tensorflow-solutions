{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Doing XYZ\n",
    "Overview of lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Colab check\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print (\"Running in Google COLAB : \", IN_COLAB)\n",
    "\n",
    "\n",
    "if IN_COLAB == True:\n",
    "    from tensorboardcolab import *\n",
    "    !pip install -U tensorboardcolab\n",
    "# Load the TensorBoard notebook extension\n",
    "    %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get Data\n",
    "Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the data\n",
    "data_location = '/data/iris/keras/iris.csv'\n",
    "# data_location = \"https://s3.amazonaws.com/elephantscale-public/data/iris/keras/iris.csv\"\n",
    "\n",
    "data = pd.read_csv(data_location)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic Exploration of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do datacleanup here\n",
    "data = data.dropna()\n",
    "\n",
    "## may be do some graphs ..etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Shape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = ['a', 'b']\n",
    "x = data [input_columns]\n",
    "y = data[['label_col']]\n",
    "\n",
    "print (x.head())\n",
    "print (y.head())\n",
    "\n",
    "## encode data if need to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test, y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0) \n",
    "\n",
    "print (\"x_train.shape : \", x_train.shape)\n",
    "print (\"y_train.shape : \", y_train.shape)\n",
    "print (\"x_test.shape : \", x_test.shape)\n",
    "print (\"y_test.shape : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=5, activation=tf.nn.relu, input_dim=input_dim, name=\"input_layer\"),\n",
    "            tf.keras.layers.Dense(units=64, activation=tf.nn.relu, name=\"hidden_1\"),\n",
    "            tf.keras.layers.Dense(units=3,  activation=tf.nn.softmax, name=\"output_layer\")\n",
    "            ])\n",
    "\n",
    "# loss = 'sparse_categorical_crossentropy'  or 'categorical_crossentropy'\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "                 optimizer=tf.keras.optimizers.Adam(), # or 'adam', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is fairly boiler plate code\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "app_name = 'sample-1' # you can change this, if you like\n",
    "\n",
    "tb_top_level_dir= '/tmp/tensorboard-logs'\n",
    "tensorboard_logs_dir= os.path.join (tb_top_level_dir, app_name, \n",
    "                                    datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))\n",
    "print (\"Saving TB logs to : \" , tensorboard_logs_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 100 \n",
    "\n",
    "print (\"training starting ...\")\n",
    "history = model.fit(\n",
    "              x_train, y_train,\n",
    "              epochs=epochs, validation_split = 0.2, verbose=1,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n",
    "print (\"training done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Plot train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Print out eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = model.metrics_names\n",
    "print (\"model metrics : \" , metric_names)\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    print (\"Metric : {} = {:,.2f}\".format (metric_names[idx], metrics[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Confusion matrix (for classification problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plain confusion matrix \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels = [0,1,2])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Discuss results and Tune the Model\n",
    "Class discussion on how the model performed and how to improve it.  \n",
    "May be add more neurons / layers ..etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
